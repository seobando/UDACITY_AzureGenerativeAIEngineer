{
  "package": {},
  "code": {
    "chat.jinja2": {
      "type": "llm",
      "inputs": {
        "context": {
          "type": [
            "string"
          ]
        },
        "question": {
          "type": [
            "string"
          ]
        },
        "chat_history": {
          "type": [
            "string"
          ]
        }
      },
      "source": "chat.jinja2"
    },
    "tools/retrieve.py": {
      "type": "python",
      "inputs": {
        "query": {
          "type": [
            "string"
          ]
        },
        "index_name": {
          "type": [
            "string"
          ]
        },
        "top_k": {
          "type": [
            "int"
          ],
          "default": "3"
        }
      },
      "description": "Retrieve relevant documents from Azure AI Search index.\n\nArgs:\n    query: The search query\n    index_name: The name of the Azure AI Search index\n    top_k: Number of documents to retrieve\n\nReturns:\n    A formatted string containing the retrieved documents",
      "source": "tools/retrieve.py",
      "function": "retrieve"
    },
    "tools/chat.py": {
      "type": "python",
      "inputs": {
        "question": {
          "type": [
            "string"
          ]
        },
        "context": {
          "type": [
            "string"
          ]
        },
        "chat_history": {
          "type": [
            "list"
          ]
        },
        "deployment_name": {
          "type": [
            "string"
          ],
          "default": "gpt-4o"
        },
        "max_tokens": {
          "type": [
            "int"
          ],
          "default": "512"
        },
        "temperature": {
          "type": [
            "double"
          ],
          "default": "0.7"
        }
      },
      "description": "Generate a chat response using Azure OpenAI with retrieved context.\n\nArgs:\n    question: The user's question\n    context: Retrieved context from Azure AI Search\n    chat_history: List of previous chat interactions\n    deployment_name: Azure OpenAI deployment name (default: gpt-4o)\n    max_tokens: Maximum tokens in response (default: 512)\n    temperature: Sampling temperature (default: 0.7)\n\nReturns:\n    The assistant's response as a string",
      "source": "tools/chat.py",
      "function": "chat"
    },
    "retrieve.py": {
      "type": "python",
      "inputs": {
        "query": {
          "type": [
            "string"
          ]
        },
        "index_name": {
          "type": [
            "string"
          ]
        },
        "top_k": {
          "type": [
            "int"
          ],
          "default": "3"
        }
      },
      "description": "Retrieve relevant documents from Azure AI Search index.\n\nArgs:\n    query: The search query\n    index_name: The name of the Azure AI Search index\n    top_k: Number of documents to retrieve\n\nReturns:\n    A formatted string containing the retrieved documents",
      "source": "retrieve.py",
      "function": "retrieve"
    },
    "chat.py": {
      "type": "python",
      "inputs": {
        "question": {
          "type": [
            "string"
          ]
        },
        "context": {
          "type": [
            "string"
          ]
        },
        "chat_history": {
          "type": [
            "list"
          ]
        },
        "deployment_name": {
          "type": [
            "string"
          ],
          "default": "gpt-4o"
        },
        "max_tokens": {
          "type": [
            "int"
          ],
          "default": "512"
        },
        "temperature": {
          "type": [
            "double"
          ],
          "default": "0.7"
        }
      },
      "description": "Generate a chat response using Azure OpenAI with retrieved context.\n\nArgs:\n    question: The user's question\n    context: Retrieved context from Azure AI Search\n    chat_history: List of previous chat interactions\n    deployment_name: Azure OpenAI deployment name (default: gpt-4o)\n    max_tokens: Maximum tokens in response (default: 512)\n    temperature: Sampling temperature (default: 0.7)\n\nReturns:\n    The assistant's response as a string",
      "source": "chat.py",
      "function": "chat"
    }
  }
}